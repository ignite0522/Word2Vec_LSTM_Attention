# Word2Vec+LSTM+Attentionæ¶æ„è¯„è®ºè¯†åˆ«



æ¥ç€ä¸Šç¯‡æ–‡ç« é—ç•™çš„é—®é¢˜ï¼Œå­¦å­¦word2vecï¼Œè¯•ç€æ¥è§£å†³ä¸€ä¸‹

## ç»Ÿè®¡è¯­è¨€æ¨¡å‹ï¼šN-gramæ¨¡å‹

#### ç®€ä»‹

åœ¨word2vecä¹‹å‰ï¼Œæˆ‘å…ˆæ¥èŠèŠN-gramæ¨¡å‹

ç®€å•æ¥è¯´ï¼Œç»Ÿè®¡è¯­è¨€æ¨¡å‹å°±æ˜¯ç”¨æ¥è®¡ç®—å¥å­æ¦‚ç‡çš„æ¦‚ç‡æ¨¡å‹



è¿™é‡Œæåˆ°å¥å­ğŸŠæ¦‚ç‡ï¼Œé‚£å¥å­æ¦‚ç‡æ˜¯å•¥å‘¢ï¼Ÿ

ä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼š

å‡è®¾ä¸€ä¸ªé•¿åº¦ä¸ºmçš„å¥å­ï¼ŒåŒ…å«è¿™äº›è¯ï¼š![(w_1,w_2,w_3,..,w_m)](assets/8LaFPhxHgewmcBr.png)ï¼Œé‚£ä¹ˆè¿™ä¸ªå¥å­çš„æ¦‚ç‡ï¼ˆä¹Ÿå°±æ˜¯è¿™![m](assets/HC6lhfLuqPrazgF.png)ä¸ªè¯å…±ç°çš„æ¦‚ç‡ï¼‰æ˜¯ï¼š

![img](assets/skwUOIXWf9RHT7F.png)



è¿˜ä¸å¤Ÿç®€å•ï¼Ÿé‚£å†ä¸¾ä¸ªæ›´å®é™…çš„ä¾‹å­

N-gramæ¨¡å‹çš„ä¸»è¦ç›®çš„æ˜¯æ•è·è¯­è¨€ä¸­çš„çŸ­è¯­ç»“æ„å’Œä¸Šä¸‹æ–‡å…³ç³»

æ¯”å¦‚

`I am going to school` æˆ– `I an going to school`

N-gram æ¨¡å‹è®¡ç®—ä¸¤ç§å¥å­çš„æ¦‚ç‡ï¼š

- P(I am going)>P(I an going)

è®¡ç®—çš„ç»“æœæ˜¯å‰è€…çš„æ¦‚ç‡æ›´å¤§ï¼Œå³ä»£è¡¨å‰ä¸€ä¸ªå¥å­æ›´ç¬¦åˆå®é™…çš„è¯­ä¹‰ç¯å¢ƒ



#### ä¸€ã€äºŒã€ä¸‰å…ƒæ¨¡å‹

è¿˜æœ‰ä¸€ä¸ªè¦æ³¨æ„çš„ç‚¹æ˜¯ï¼šç»å¸¸æåˆ°æ˜¯å‡ å…ƒæ¨¡å‹ï¼Œè¿™é‡Œçš„å‡ å…ƒæ˜¯å•¥æ„æ€ï¼Ÿ

å½“ n=1, ä¸€ä¸ªä¸€å…ƒæ¨¡å‹ï¼ˆunigram model)å³ä¸º ï¼šï¼ˆè¿™æ˜¯ä¸€ä¸ªç‰¹æ®Šæƒ…å†µï¼Œåªè€ƒè™‘å•ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡ï¼‰

![img](assets/uRbKyoYzVavtflL.png)

å½“ n=2, ä¸€ä¸ªäºŒå…ƒæ¨¡å‹ï¼ˆbigram model)å³ä¸º ï¼š

![img](assets/2MdOFhGs7v6V1ty.png)

å½“ n=3, ä¸€ä¸ªä¸‰å…ƒæ¨¡å‹ï¼ˆ[trigram model](https://zhida.zhihu.com/search?content_id=5320991&content_type=Article&match_order=1&q=trigram+model&zhida_source=entity))å³ä¸º

![img](assets/QzdrcMmWPTx2fp8.png)



æ¥ä¸‹æ¥è®²è®²äºŒå…ƒæ¨¡å‹ï¼š

ä¸¾ä¸ªä¾‹å­ï¼šå‡è®¾æˆ‘ä»¬æœ‰ä¸ªè¯­æ–™åº“ï¼Œæˆ‘ä»¬å¯¹è¯è¯­è¿›è¡Œæ„å»ºäºŒå…ƒå…³ç³»

<img src="assets/ajdPzpc5KlRTEs8.png" alt="img" style="zoom: 200%;" />

å…¶ä¸­ç¬¬ä¸€è¡Œï¼Œç¬¬äºŒåˆ— è¡¨ç¤ºç»™å®šå‰ä¸€ä¸ªè¯æ˜¯ â€œiâ€ æ—¶ï¼Œå½“å‰è¯ä¸ºâ€œwantâ€çš„æƒ…å†µä¸€å…±å‡ºç°äº†827æ¬¡



æ®æ­¤ï¼Œæˆ‘ä»¬ä¾¿å¯ä»¥ç®—å¾—ç›¸åº”çš„é¢‘ç‡åˆ†å¸ƒè¡¨å¦‚ä¸‹ã€‚

<img src="assets/e1iT5VxEMZq3YCU.png" alt="img" style="zoom:200%;" />

æ¯”å¦‚ï¼Œå‰ä¸€ä¸ªå•è¯æ˜¯iï¼Œé‚£ä¹ˆä¸‹ä¸€ä¸ªå•è¯æ˜¯wantçš„æ¦‚ç‡ä¸º0.33ï¼Œä¸‹ä¸€ä¸ªå•è¯æ˜¯eatçš„æ¦‚ç‡æ˜¯0.0036



çœ‹åˆ°è¿™ä¸ªæ˜¯ä¸æ˜¯ä¸€ä¸‹å°±æƒ³èµ·äº†ä½ åœ¨æµè§ˆèµ·æœç´¢æ—¶é‡åˆ°çš„æƒ…å†µï¼š

ä½ åœ¨ç”¨è°·æ­Œæ—¶ï¼Œè¾“å…¥ä¸€ä¸ªæˆ–å‡ ä¸ªè¯ï¼Œ**æœç´¢æ¡†é€šå¸¸ä¼šä»¥ä¸‹æ‹‰èœå•çš„å½¢å¼ç»™å‡ºå‡ ä¸ªåƒä¸‹å›¾ä¸€æ ·çš„å¤‡é€‰ï¼Œè¿™äº›å¤‡é€‰å…¶å®æ˜¯åœ¨çŒœæƒ³ä½ æƒ³è¦æœç´¢çš„é‚£ä¸ªè¯ä¸²ã€‚**

![img](assets/f5GAyrQTvWPCRp9.png)

è¿™å…¶å®å°±æ˜¯ä»¥N-Gramæ¨¡å‹ä¸ºåŸºç¡€æ¥å®ç°çš„



#### å±€é™

ä½†è¿™ä¸ªæ¨¡å‹å…·æœ‰å¾ˆå¤§çš„å±€é™æ€§ï¼š

é¦–å…ˆå®ƒè€ƒè™‘å½“å‰è¯æ—¶ï¼Œå½“å‰è¯åªä¸è·ç¦»å®ƒæ¯”è¾ƒè¿‘çš„nä¸ªè¯æ›´åŠ ç›¸å…³(ä¸€èˆ¬nä¸è¶…è¿‡3)ï¼Œè€Œéå‰é¢æ‰€æœ‰çš„è¯éƒ½æœ‰å…³

å…¶æ¬¡ï¼Œå®ƒæ²¡æœ‰è€ƒè™‘è¯ä¸è¯ä¹‹é—´å†…åœ¨çš„è”ç³»æ€§ï¼Œæ­¤è¯æ€è®²ï¼Ÿ

```
ä¾‹å¦‚ï¼Œè€ƒè™‘"the cat is walking in the bedroom"è¿™å¥è¯
å¦‚æœæˆ‘ä»¬åœ¨è®­ç»ƒè¯­æ–™ä¸­çœ‹åˆ°äº†å¾ˆå¤šç±»ä¼¼â€œthe dog is walking in the bedroomâ€æˆ–æ˜¯â€œthe cat is running in the bedroomâ€è¿™æ ·çš„å¥å­ï¼Œé‚£ä¹ˆï¼Œå“ªæ€•æˆ‘ä»¬æ­¤å‰æ²¡æœ‰è§è¿‡è¿™å¥è¯"the cat is walking in the bedroom"ï¼Œä¹Ÿå¯ä»¥ä»â€œcatâ€å’Œâ€œdogâ€ï¼ˆâ€œwalkingâ€å’Œâ€œrunningâ€ï¼‰ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæ¨æµ‹å‡ºè¿™å¥è¯çš„æ¦‚ç‡
ä½†N-Gramåšä¸åˆ°è¿™ç‚¹
```





## Word2Vec

å…ˆæ¥èŠèŠä¸ºå•¥ä¼šå‡ºç°Word2Vec

åœ¨è¿™ä¹‹å‰æ˜¯å­˜åœ¨ä¼ ç»Ÿçš„one-hot ç¼–ç ï¼Œä½†ä¼ ç»Ÿçš„one-hot ç¼–ç ä»…ä»…åªæ˜¯å°†è¯ç¬¦å·åŒ–ï¼Œä¸åŒ…å«ä»»ä½•è¯­ä¹‰ä¿¡æ¯ï¼Œè¿˜æœ‰ä¸ªæœ€å¤§çš„ç—›ç‚¹å°±æ˜¯è¯çš„ç‹¬çƒ­è¡¨ç¤ºï¼ˆone-hot representationï¼‰æ˜¯é«˜ç»´çš„ï¼Œä¸”åœ¨é«˜ç»´å‘é‡ä¸­åªæœ‰ä¸€ä¸ªç»´åº¦æè¿°äº†è¯çš„è¯­ä¹‰ (é«˜åˆ°ä»€ä¹ˆç¨‹åº¦å‘¢ï¼Ÿè¯å…¸æœ‰å¤šå¤§å°±æœ‰å¤šå°‘ç»´ï¼Œä¸€èˆ¬è‡³å°‘ä¸Šä¸‡çš„ç»´åº¦)ï¼Œè¿™æ˜¯æ¨¡å‹è®­ç»ƒæœ€ä¸èƒ½å¿å—çš„ï¼Œç»´åº¦æé«˜ä½†æœ‰ç”¨ä¿¡æ¯åˆæå…¶å°‘ï¼Œå¯ä»¥è¯´æ˜¯åˆé•¿åˆè‡­

æ‰€ä»¥ç›®å‰è¦è§£å†³çš„å°±æ˜¯

1.èµ‹äºˆè¯è¯­è¯­ä¹‰ä¿¡æ¯

2.é™ä½ç»´åº¦



word2vecå°±æ¨ªç©ºå‡ºä¸–äº†

**ç”¨word2vecè®­ç»ƒå‡ºæ¥çš„è¯å‘é‡çŸ©é˜µï¼Œè¯ä¸è¯ä¹‹é—´æ˜¯å­˜åœ¨è¯­ä¹‰å…³ç³»çš„ï¼Œè€Œä¸”å¯ä»¥å°†è¯å‘é‡çš„çº¬åº¦ä»å‡ åƒå‡ ä¸‡ç›´æ¥é™åˆ°å‡ ç™¾**



#### ç»“æ„

word2vecåŒ…å«ä¸‰å±‚ï¼šè¾“å…¥å±‚ã€éšè—å±‚ã€è¾“å‡ºå±‚ï¼Œ**é€šè¿‡ä»è¾“å…¥å±‚åˆ°éšè—å±‚æˆ–éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡çŸ©é˜µå»å‘é‡åŒ–è¡¨ç¤ºè¯çš„è¾“å…¥ï¼Œå­¦ä¹ è¿­ä»£çš„æ˜¯ä¸¤ä¸ªæƒé‡çŸ©é˜µ**ï¼Œå¦‚ä¸‹å›¾ï¼š

![img](assets/GqkKrdnSTmQC8bR.png)





æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±ä¸€èµ·çœ‹çœ‹word2vecçš„è®­ç»ƒè¿‡ç¨‹(ä¸‹é¢ä»¥çŸ¥ä¹ç½‘å‹ crystalajj æä¾›çš„ PPT ä¸ºä¾‹çœ‹ä¸€ä¸‹ CBOW æ¨¡å‹è®­ç»ƒæµç¨‹)

ç¤ºä¾‹å¥å­ï¼š**I drink coffee everyday**



1.å°†ä¸Šä¸‹æ–‡è¯è¿›è¡Œ one-hot è¡¨å¾ä½œä¸ºè¾“å…¥ï¼š

```
Iï¼š        [1,0,0,0]
drinkï¼š     [0,1,0,0]
coffeeï¼š    ï¼Ÿ
everydayï¼š [0,0,0,1]
```

![img](assets/8kzhecL274snQSB.png)



2.ç„¶åå°† one-hot è¡¨å¾ç»“æœ[1,0,0,0]ã€[**0,1,0,0**]ã€[0,0,0,1]ï¼Œåˆ†åˆ«ä¹˜ä»¥ï¼š3Ã—4çš„è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡çŸ©é˜µWã€Œè¿™ä¸ªçŸ©é˜µä¹Ÿå«åµŒå…¥çŸ©é˜µï¼Œå¯ä»¥éšæœºåˆå§‹åŒ–ç”Ÿæˆã€

![img](assets/nLbDq5oa9rRwBmX.png)

åˆ°è¿™é‡Œå¯ä»¥çœ‹åˆ°ï¼Œç»´åº¦å·²ç»å‡ä¸‹æ¥äº†ï¼Œè¿™è¿˜ä¸æ˜æ˜¾ï¼Œå¦‚æœè¯å…¸ä¸­çš„å•è¯ä¸ºä¸Šä¸‡ä¸ªï¼Œé‚£ä¸€ä¸‹å‹ç¼©åˆ°å‡ ç™¾ä¸ªå°±æ˜æ˜¾äº†

**ä¸¾ä¾‹ï¼šæ¯”å¦‚ä¸Šå›¾ä¸­ï¼Œè‹¥è¯å…¸ä¸­æœ‰10000ä¸ªå•è¯ï¼Œé‚£ä¹ˆæ¯ä¸ªå•è¯çš„ç‹¬çƒ­ç¼–ç å°±æ˜¯10000\*1ï¼Œè¿™ä¸ªç»´åº¦ä¸º10000**

**ç¬¬ä¸€ä¸ªçŸ©é˜µWçš„å½¢çŠ¶ä¸º200\*10000ï¼Œç¬¬äºŒä¸ªçŸ©é˜µ(å•è¯)å½¢çŠ¶ä¸º10000\*1,ç›¸ä¹˜è¿‡åçš„çŸ©é˜µå½¢çŠ¶ä¸º200\*1ï¼Œç»´åº¦ç›´æ¥æ–­å´–å¼ä¸‹é™**



3.å°†å¾—åˆ°çš„ç»“æœå‘é‡æ±‚å¹³å‡ä½œä¸ºéšè—å±‚å‘é‡ï¼š[1, 1.67, 0.33]

![img](assets/9gRKv6JWLz8qorp.png)





4.ç„¶åå°†éšè—å±‚[1, 1.67, 0.33]å‘é‡ä¹˜ä»¥ï¼š4Ã—3çš„éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡çŸ©é˜µ![W'](assets/ovcauxJZAHq6SBr.png)ã€Œè¿™ä¸ªçŸ©é˜µä¹Ÿæ˜¯åµŒå…¥çŸ©é˜µï¼Œä¹Ÿå¯ä»¥åˆå§‹åŒ–å¾—åˆ°ã€ï¼Œå¾—åˆ°è¾“å‡ºå‘é‡ï¼š[4.01, 2.01, 5.00, 3.34]

![img](assets/uwkKHXL5U4oSzJP.png)



5.æœ€åå¯¹è¾“å‡ºå‘é‡[4.01, 2.01, 5.00, 3.34] åš softmax æ¿€æ´»å¤„ç†å¾—åˆ°å®é™…è¾“å‡º[0.23, 0.03, 0.62, 0.12]ï¼Œå¹¶å°†å…¶ä¸çœŸå®æ ‡ç­¾[0, 0, 1, 0]åšæ¯”è¾ƒï¼Œç„¶ååŸºäºæŸå¤±å‡½æ•°åšæ¢¯åº¦ä¼˜åŒ–è®­ç»ƒ

![img](assets/lJYgmxOXS6haptV.png)



è¿™ä¸€ç³»åˆ—å›¾æ—¶çœŸçš„å¤ªè¯¦ç»†äº†ï¼Œæ„Ÿè°¢ä½œè€…

æœ€åè¿˜ç»™å‡ºäº†å®Œæ•´å›¾

![img](assets/jPcfN9Gyh738HrD.png)



#### æ•ˆæœå±•ç¤º

ä¸Šè¿°è¿‡ç¨‹æˆ‘ä»¬ç›´è§‚çš„çœ‹åˆ°äº†word2vecæ˜¯æ€ä¹ˆé™ä½ç»´åº¦å’Œèµ‹äºˆæ¬¡ä¹‹é—´å…³è”æ€§

é‚£æ•ˆæœå¦‚ä½•å‘¢ï¼Ÿä¸ºäº†æ–¹ä¾¿å±•ç¤ºï¼Œè¿™é‡Œæœ‰ä¸€å¼ å°†128ç»´å‹ç¼©æˆ2ç»´çš„å›¾

![img](assets/Oo2vZpt1crBqs3N.png)

å¯ä»¥çœ‹åˆ°æ„æ€ç›¸è¿‘æˆ–è€…è¯æ€§ç›¸åŒçš„è¯è¯­ä¹‹é—´çš„è·ç¦»å¾ˆè¿‘



å¦‚æœå›¾éƒ½ä¸èƒ½æ»¡è¶³ä½ å¯¹word2vecçš„è®¤çŸ¥æ¬²æœ›çš„è¯ï¼Œé‚£æˆ‘ä»¬å°±æ¥è·‘è·‘ä»£ç çœ‹çœ‹ï¼ï¼

è€è§„çŸ©ï¼Œå…ˆä¸Šä»£ç 

```py
import os
from gensim.models import Word2Vec
import re

# æ¸…æ´—æ–‡æœ¬ï¼Œå»é™¤æ•°å­—å’Œæ ‡ç‚¹ç¬¦å·
def clean_text(text):
    text = re.sub(r'\d+', '', text)  # å»é™¤æ•°å­—
    text = re.sub(r'[^\w\s]', '', text)  # å»é™¤æ ‡ç‚¹ç¬¦å·
    return text

# åˆ†è¯å·¥å…·
def tokenize(text):
    text = clean_text(text)  # åœ¨åˆ†è¯ä¹‹å‰å…ˆæ¸…æ´—æ–‡æœ¬
    return text.split()  # å‡è®¾è¾“å…¥æ–‡æœ¬å·²ç»æŒ‰ç©ºæ ¼åˆ†è¯ã€‚å¦‚æœæ˜¯ä¸­æ–‡ï¼Œè¯·ä½¿ç”¨ jieba æˆ–å…¶ä»–åˆ†è¯å·¥å…·ã€‚

# åŠ è½½æ•°æ®
def load_files_from_dir(directory):
    sentences = []
    for label in ["pos", "neg"]:
        label_dir = os.path.join(directory, label)
        for file_name in os.listdir(label_dir):
            with open(os.path.join(label_dir, file_name), "r", encoding="utf-8") as file:
                sentences.append(tokenize(file.read()))  # åˆ†è¯åå­˜å‚¨ä¸ºåˆ—è¡¨
    return sentences

def load_all_files():
    train_texts = load_files_from_dir("./aclImdb/train")
    test_texts = load_files_from_dir("./aclImdb/test")
    return train_texts, test_texts

# è®­ç»ƒ Word2Vec æ¨¡å‹
def train_word2vec(sentences, vector_size=100, window=5, min_count=2, workers=4):
    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)
    return model

# ä¿å­˜å’ŒåŠ è½½æ¨¡å‹
def save_model(model, path="word2vec.model"):
    model.save(path)
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {path}")

def load_model(path="word2vec.model"):
    return Word2Vec.load(path)

# ä¸»å‡½æ•°
if __name__ == "__main__":
    print("åŠ è½½æ•°æ®ä¸­...")
    train_texts, test_texts = load_all_files()
    print(f"è®­ç»ƒæ•°æ®ï¼š{len(train_texts)} æ¡ï¼Œæµ‹è¯•æ•°æ®ï¼š{len(test_texts)} æ¡")

    print("è®­ç»ƒ Word2Vec æ¨¡å‹ä¸­...")
    model = train_word2vec(train_texts, vector_size=100, window=5, min_count=2, workers=4)

    save_model(model, "./model/word2vec.model")

    print("åŠ è½½ä¿å­˜çš„æ¨¡å‹...")
    loaded_model = load_model("./model/word2vec.model")
    print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼è¯æ±‡è¡¨å¤§å°ï¼š{len(loaded_model.wv)}")

```

æœ¬æ¬¡ä½¿ç”¨çš„æ•°æ®é›†æ¥è‡ªäº’è”ç½‘ç”µå½±èµ„æ–™åº“ï¼ˆInternet Movie Databaseï¼ŒIMDBï¼‰ï¼ŒIMDBæ˜¯ä¸€ä¸ªå…³äºç”µå½±æ¼”å‘˜ã€ç”µå½±ã€ç”µè§†èŠ‚ç›®ã€ç”µè§†æ˜æ˜Ÿå’Œç”µå½±åˆ¶ä½œçš„åœ¨çº¿æ•°æ®åº“ã€‚

è®­ç»ƒå’Œæµ‹è¯•æ•°æ®å„25000æ¡

æˆ‘ä»¬å…ˆéšä¾¿çœ‹ä¸€æ¡æ­£é¢è¯„è®º

```
If you like adult comedy cartoons, like South Park, then this is nearly a similar format about the small adventures of three teenage girls at Bromwell High. Keisha, Natella and Latrina have given exploding sweets and behaved like bitches, I think Keisha is a good leader. There are also small stories going on with the teachers of the school. There's the idiotic principal, Mr. Bip, the nervous Maths teacher and many others. The cast is also fantastic, Lenny Henry's Gina Yashere, EastEnders Chrissie Watts, Tracy-Ann Oberman, Smack The Pony's Doon Mackichan, Dead Ringers' Mark Perry and Blunder's Nina Conti. I didn't know this came from Canada, but it is very good. Very good!
```



å¯ä»¥çœ‹åˆ°æœ‰æ ‡ç‚¹ç¬¦å·ï¼Œæ•°å­—ä¹‹ç±»çš„

åœ¨è®­ç»ƒä¹‹å‰å…ˆæŠŠæ•°æ®æ¸…æ´—ä¸€ä¸‹ï¼Œå»å¤„æ–‡æœ¬ä¸­çš„æ•°å­—å’Œæ ‡ç‚¹ç¬¦å·ï¼ˆå› ä¸ºå®ƒä»¬å¯¹äºè¯­ä¹‰æ— å®é™…æ„ä¹‰ï¼‰

```py
# æ¸…æ´—æ–‡æœ¬ï¼Œå»é™¤æ•°å­—å’Œæ ‡ç‚¹ç¬¦å·
def clean_text(text):
    text = re.sub(r'\d+', '', text)  # å»é™¤æ•°å­—
    text = re.sub(r'[^\w\s]', '', text)  # å»é™¤æ ‡ç‚¹ç¬¦å·
    return text
```



æ¥ç€è¦å°†æ¯ä¸ªè¯„è®ºæ‹†è§£æˆä¸€ä¸ªåˆ—è¡¨

```py
# åˆ†è¯å·¥å…·
def tokenize(text):
    text = clean_text(text)  # åœ¨åˆ†è¯ä¹‹å‰å…ˆæ¸…æ´—æ–‡æœ¬
    return text.split()  # å‡è®¾è¾“å…¥æ–‡æœ¬å·²ç»æŒ‰ç©ºæ ¼åˆ†è¯ã€‚å¦‚æœæ˜¯ä¸­æ–‡ï¼Œè¯·ä½¿ç”¨ jieba æˆ–å…¶ä»–åˆ†è¯å·¥å…·ã€‚
```



```Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)```å³ï¼š

vector_sizeï¼šå¾—åˆ°çš„å‘é‡ç»´åº¦ä¸º100

Windowsï¼šçª—å£å¤§å°ï¼Œå†³å®šæ¯ä¸ªè¯ä¸Šä¸‹æ–‡çš„èŒƒå›´ï¼ˆé»˜è®¤5ï¼‰

min_countï¼šå¿½ç•¥å‡ºç°é¢‘ç‡ä½äºæ­¤å€¼çš„è¯

workersï¼š  å¹¶è¡Œçº¿ç¨‹æ•°ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒ



åˆ‡è®°ä¿å­˜è®­ç»ƒå¥½çš„è¯å‘é‡ï¼Œæˆ‘è®­ç»ƒçš„å­—å…¸ä¸­çš„è¯å°‘ï¼Œæ²¡ä¿å­˜é‡æ–°è·‘å‡ åˆ†é’Ÿå°±å¯ä»¥å•¦ï¼Œé‡åˆ°è¶…çº§å¤§çš„è¯å…¸è®­ç»ƒçš„æ—¶é—´å°±æ˜¯ä»¥å¤©è®¡ç®—äº†



æ—¢ç„¶è®­ç»ƒå¥½äº†ï¼Œé‚£å°±æ¥çœ‹çœ‹å§

```py
from gensim.models import Word2Vec

# åŠ è½½æ¨¡å‹
loaded_model = Word2Vec.load("./model/word2vec.model")

# è®¿é—®è¯æ±‡è¡¨
vocabulary = loaded_model.wv.index_to_key
print("è¯æ±‡è¡¨ï¼š", vocabulary[:10])  # æ‰“å°å‰ 10 ä¸ªå•è¯


similarity = loaded_model.wv.similarity("good", "great")
print("\nç›¸ä¼¼åº¦ (good vs great):", similarity)

similarity = loaded_model.wv.similarity("love", "great")
print("\nç›¸ä¼¼åº¦ (love vs great):", similarity)

similar_words = loaded_model.wv.most_similar("love", topn=5)
print("\nä¸ 'love' æœ€ç›¸ä¼¼çš„å•è¯ï¼š", similar_words)


# è®¿é—®å•è¯çš„è¯å‘é‡
word = "love"
if word in loaded_model.wv:
    print(f"\nå•è¯ '{word}' å­˜åœ¨äºè¯æ±‡è¡¨ä¸­")
    # è·å–è¯¥å•è¯çš„è¯å‘é‡
    vector = loaded_model.wv[word]
    print(f"\n'{word}' çš„è¯å‘é‡ï¼š", vector)
else:
    print(f"\nå•è¯ '{word}' ä¸åœ¨è¯æ±‡è¡¨ä¸­")

# æŸ¥çœ‹è¯å‘é‡çš„ç»´åº¦
vector_size = loaded_model.wv.vector_size
print(f"\nè¯å‘é‡çš„ç»´åº¦æ˜¯ï¼š{vector_size}")
```



çœ‹çœ‹ç»“æœ

![image-20250103160823792](assets/Zb32rMSnRJTQ5NE.png)



åœ¨ç›¸ä¼¼åº¦ä¸Šï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†goodå’Œgreatæ¥æ¯”è¾ƒï¼Œloveå’Œgreatæ¥æ¯”è¾ƒ

goodå’Œgreatçš„ç›¸ä¼¼åº¦çº¦ä¸º0.76ï¼Œå¯ä»¥çœ‹åˆ°æ˜¯æ¯”è¾ƒç›¸ä¼¼äº†

loveå’Œgreatçš„ç›¸ä¼¼åº¦çº¦ä¸º0.33ï¼Œä¸ç›¸ä¼¼



åœ¨æ‰¾ä¸loveç›¸ä¼¼çš„å•è¯æ—¶

å¯ä»¥çœ‹åˆ°å…¶ä¸­hateçš„ç›¸ä¼¼åº¦æœ€é«˜ï¼Œç«Ÿç„¶æ˜¯åä¹‰è¯æ¨æœ€é«˜ï¼ï¼Ÿ

æ¥ç€å°±æ˜¯å®ƒçš„å½¢å®¹è¯lovedï¼Œåœ¨æ¥ç€å°±æ˜¯enjoy



**è¿™ä¹Ÿå†æ¬¡éªŒè¯äº†word2vecæ˜¯å¯ä»¥èµ‹äºˆè¯ä¸è¯ä¹‹é—´çš„è¯­ä¹‰çš„**

**æœ€åä¹Ÿæ˜¯æœ€é‡è¦çš„ä¸€ç‚¹ï¼Œæ¯ä¸ªè¯çš„ç»´åº¦ä»25000æ–­å´–å¼ä¸‹é™åˆ°100ï¼Œè¿™å¯ä»¥ç§°å¾—ä¸Šæ˜¯æ¨¡å‹è®­ç»ƒä¸Šçš„ä¸€å¤§æ­¥å•Šï¼Œå‡å°‘ä¸¤å¾ˆå¤šæ— ç”¨ä¸”åºå¤§çš„è®¡ç®—é‡**







## æ¶æ„è¯„è®ºè¯†åˆ«å®æˆ˜



### ä¸€å±‚LSTM

å…ˆä¸Šä»£ç 

```py
import os
import re
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
from gensim.models import Word2Vec
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

# æ¸…æ´—æ–‡æœ¬
def clean_text(text):
    text = re.sub(r'\d+', '', text)  # å»é™¤æ•°å­—
    text = re.sub(r'[^\w\s]', '', text)  # å»é™¤æ ‡ç‚¹ç¬¦å·
    text = text.lower()  # è½¬ä¸ºå°å†™
    return text

# åŠ è½½æ•°æ®
def load_files_from_dir(directory):
    texts = []
    labels = []
    for label in ["pos", "neg"]:
        label_dir = os.path.join(directory, label)
        for file_name in os.listdir(label_dir):
            with open(os.path.join(label_dir, file_name), "r", encoding="utf-8") as file:
                text = file.read()
                cleaned_text = clean_text(text)
                texts.append(cleaned_text)
                labels.append(1 if label == "pos" else 0)  # æ­£ç±»ä¸º 1ï¼Œè´Ÿç±»ä¸º 0
    return texts, labels

def load_all_files():
    train_texts, train_labels = load_files_from_dir("./aclImdb/train")
    test_texts, test_labels = load_files_from_dir("./aclImdb/test")
    return train_texts, train_labels, test_texts, test_labels

# åˆ›å»ºåµŒå…¥çŸ©é˜µ
def create_embedding_matrix(word_index, word2vec_model, embedding_dim):
    vocab_size = len(word_index) + 1  # +1 å› ä¸ºç´¢å¼•ä» 1 å¼€å§‹ï¼Œ0 æ˜¯ç”¨äºå¡«å……çš„
    embedding_matrix = np.zeros((vocab_size, embedding_dim))

    for word, i in word_index.items():
        if word in word2vec_model.wv:
            embedding_matrix[i] = word2vec_model.wv[word]

    return embedding_matrix

# ä¸»ç¨‹åº
if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    train_texts, train_labels, test_texts, test_labels = load_all_files()

    # æ•°æ®é¢„å¤„ç†
    MAX_NUM_WORDS = 10000  # è¯æ±‡è¡¨å¤§å°
    MAX_SEQ_LEN = 100  # æ¯ä¸ªå¥å­çš„æœ€å¤§é•¿åº¦

    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token="<OOV>")
    tokenizer.fit_on_texts(train_texts)

    X_train = tokenizer.texts_to_sequences(train_texts)
    X_test = tokenizer.texts_to_sequences(test_texts)

    # å¡«å……åºåˆ—åˆ°å›ºå®šé•¿åº¦
    X_train = pad_sequences(X_train, maxlen=MAX_SEQ_LEN, padding="post", truncating="post")
    X_test = pad_sequences(X_test, maxlen=MAX_SEQ_LEN, padding="post", truncating="post")

    y_train = np.array(train_labels)
    y_test = np.array(test_labels)

    # åŠ è½½é¢„è®­ç»ƒçš„ Word2Vec æ¨¡å‹
    word2vec_model = Word2Vec.load("./model/word2vec.model")
    EMBEDDING_DIM = word2vec_model.vector_size


    # åˆ›å»ºåµŒå…¥çŸ©é˜µ
    embedding_matrix = create_embedding_matrix(tokenizer.word_index, word2vec_model, EMBEDDING_DIM)

    # åˆ›å»º LSTM æ¨¡å‹ï¼Œä½¿ç”¨ Word2Vec åµŒå…¥
    model = Sequential([
        Embedding(
            input_dim=embedding_matrix.shape[0],  # è¯æ±‡è¡¨å¤§å°
            output_dim=EMBEDDING_DIM,  # åµŒå…¥ç»´åº¦
            weights=[embedding_matrix],  # ä½¿ç”¨é¢„è®­ç»ƒåµŒå…¥çŸ©é˜µ
            input_length=MAX_SEQ_LEN,  # è¾“å…¥åºåˆ—é•¿åº¦
            trainable=False  # å†»ç»“åµŒå…¥å±‚
        ),
        LSTM(128, return_sequences=False),
        Dropout(0.5),
        Dense(64, activation="relu"),
        Dropout(0.5),
        Dense(1, activation="sigmoid")  # äºŒåˆ†ç±»
    ])

    model.compile(
        optimizer="adam",
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )

    # è®­ç»ƒæ¨¡å‹
    BATCH_SIZE = 32
    EPOCHS = 5

    print("å¼€å§‹è®­ç»ƒ LSTM æ¨¡å‹...")
    history = model.fit(
        X_train, y_train,
        validation_split=0.2,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        verbose=1
    )

    # æµ‹è¯•æ¨¡å‹
    print("\nè¯„ä¼°æ¨¡å‹...")
    y_pred = (model.predict(X_test) > 0.5).astype("int32")

    print("\nåˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(y_test, y_pred))

    print("\næ··æ·†çŸ©é˜µï¼š")
    print(confusion_matrix(y_test, y_pred))
```





é¦–å…ˆæ˜¯æ•°æ®é¢„å¤„ç†

å°†æ¯ä¸ªæ–‡æœ¬ä¸­çš„å•è¯è½¬æ¢æˆå¯¹åº”çš„æ•´æ•°åºå·å¹¶è£å‰ªæ¯ä¸ªæ–‡æœ¬ä½¿å¾—æ‰€æœ‰æ–‡æœ¬çš„é•¿åº¦ä¸€è‡´ï¼Œä»¥è‡³äºèƒ½è¾“å…¥ç¥ç»ç½‘ç»œ

```py

    # æ•°æ®é¢„å¤„ç†
    MAX_NUM_WORDS = 10000  # è¯æ±‡è¡¨å¤§å°
    MAX_SEQ_LEN = 100  # æ¯ä¸ªå¥å­çš„æœ€å¤§é•¿åº¦

    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token="<OOV>")
    tokenizer.fit_on_texts(train_texts)

    X_train = tokenizer.texts_to_sequences(train_texts)
    X_test = tokenizer.texts_to_sequences(test_texts)

    # å¡«å……åºåˆ—åˆ°å›ºå®šé•¿åº¦
    X_train = pad_sequences(X_train, maxlen=MAX_SEQ_LEN, padding="post", truncating="post")
    X_test = pad_sequences(X_test, maxlen=MAX_SEQ_LEN, padding="post", truncating="post")
```



æ¥ç€å°±æ˜¯åœ¨è®­ç»ƒå¥½çš„word2vecåŸºç¡€ä¸Šå¾—åˆ°åµŒå…¥çŸ©é˜µ

```
åµŒå…¥çŸ©é˜µæ˜¯ä¸€ä¸ªäºŒç»´çŸ©é˜µï¼Œå…¶ä¸­æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªè¯æ±‡è¡¨ä¸­çš„å•è¯çš„å‘é‡è¡¨ç¤ºã€‚
åµŒå…¥çŸ©é˜µçš„ç»´åº¦æ˜¯ V Ã— dï¼Œå…¶ä¸­ï¼š
V æ˜¯è¯æ±‡è¡¨å¤§å°ï¼ˆå³è¯æ±‡è¡¨ä¸­å•è¯çš„ä¸ªæ•°ï¼‰ã€‚
d æ˜¯è¯åµŒå…¥çš„ç»´åº¦ï¼ˆå³æ¯ä¸ªå•è¯å‘é‡çš„é•¿åº¦ï¼‰ã€‚
```

```py
def create_embedding_matrix(word_index, word2vec_model, embedding_dim):
    vocab_size = len(word_index) + 1  # +1 å› ä¸ºç´¢å¼•ä» 1 å¼€å§‹ï¼Œ0 æ˜¯ç”¨äºå¡«å……çš„
    embedding_matrix = np.zeros((vocab_size, embedding_dim))

    for word, i in word_index.items():
        if word in word2vec_model.wv:
            embedding_matrix[i] = word2vec_model.wv[word]

    return embedding_matrix
```



ç°åœ¨æ¯ä¸ªæ–‡æœ¬ä¸­å•è¯çš„ç»´åº¦éƒ½å¾ˆé«˜ï¼Œåœ¨è¿›è¡Œlstmè®­ç»ƒä¹‹å‰ï¼Œè¦å°†ç¦»æ•£çš„å•è¯æ˜ å°„åˆ°è¿ç»­çš„å‘é‡ç©ºé—´ä¸­çš„è¿‡ç¨‹ã€‚æ¯ä¸ªå•è¯ç”¨ä¸€ä¸ªå›ºå®šå¤§å°çš„å‘é‡è¡¨ç¤ºï¼ˆè¿™é‡Œå°±ç›¸å½“äºå°†å•è¯æ˜ å°„ä¸ºæå‰è®­ç»ƒå¥½çš„word2vecè¯å‘é‡ï¼‰

`Embedding` å±‚ï¼Œç”¨äºå°†è¯æ±‡è¡¨ä¸­çš„å•è¯è½¬åŒ–ä¸ºå¯¹åº”çš„è¯åµŒå…¥ï¼ˆå³è¯å‘é‡ï¼‰

```py
Embedding(
    input_dim=embedding_matrix.shape[0],  # è¯æ±‡è¡¨å¤§å°
    output_dim=EMBEDDING_DIM,  # åµŒå…¥ç»´åº¦
    weights=[embedding_matrix],  # ä½¿ç”¨é¢„è®­ç»ƒåµŒå…¥çŸ©é˜µ
    input_length=MAX_SEQ_LEN,  # è¾“å…¥åºåˆ—é•¿åº¦
    trainable=False  # å†»ç»“åµŒå…¥å±‚,è¡¨ç¤ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šæ›´æ–°åµŒå…¥å±‚çš„æƒé‡ï¼Œè€Œæ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡
)
```







æ¨¡å‹æ ¸å¿ƒ

```py
    model = Sequential([
        Embedding(
            input_dim=embedding_matrix.shape[0],  # è¯æ±‡è¡¨å¤§å°
            output_dim=EMBEDDING_DIM,  # åµŒå…¥ç»´åº¦
            weights=[embedding_matrix],  # ä½¿ç”¨é¢„è®­ç»ƒåµŒå…¥çŸ©é˜µ
            input_length=MAX_SEQ_LEN,  # è¾“å…¥åºåˆ—é•¿åº¦
            trainable=False  # å†»ç»“åµŒå…¥å±‚
        ),
        LSTM(128, return_sequences=False),
        Dropout(0.5),
        Dense(64, activation="relu"),
        Dropout(0.5),
        Dense(1, activation="sigmoid")  # äºŒåˆ†ç±»
    ])
```

åŒ…å«ä¸€ä¸ªåµŒå…¥å±‚ï¼ŒLSTMå±‚ï¼Œä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼Œä¸¤ä¸ªDropout å±‚



æœ€åæ¥çœ‹çœ‹æ•ˆæœå§ï¼

![image-20250103171601975](assets/VeXfEjYGPWrB9LZ.png)

æœ‰76%çš„æ­£ç¡®ç‡





### åŒå±‚LSTM

```py
import os
import re
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
from gensim.models import Word2Vec
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

# æ¸…æ´—æ–‡æœ¬
def clean_text(text):
    text = re.sub(r'\d+', '', text)  # å»é™¤æ•°å­—
    text = re.sub(r'[^\w\s]', '', text)  # å»é™¤æ ‡ç‚¹ç¬¦å·
    text = text.lower()  # è½¬ä¸ºå°å†™
    return text

# åŠ è½½æ•°æ®
def load_files_from_dir(directory):
    texts = []
    labels = []
    for label in ["pos", "neg"]:
        label_dir = os.path.join(directory, label)
        for file_name in os.listdir(label_dir):
            with open(os.path.join(label_dir, file_name), "r", encoding="utf-8") as file:
                text = file.read()
                cleaned_text = clean_text(text)
                texts.append(cleaned_text)
                labels.append(1 if label == "pos" else 0)  # æ­£ç±»ä¸º 1ï¼Œè´Ÿç±»ä¸º 0
    return texts, labels

def load_all_files():
    train_texts, train_labels = load_files_from_dir("./aclImdb/train")
    test_texts, test_labels = load_files_from_dir("./aclImdb/test")
    return train_texts, train_labels, test_texts, test_labels

# åˆ›å»ºåµŒå…¥çŸ©é˜µ
def create_embedding_matrix(word_index, word2vec_model, embedding_dim):
    vocab_size = len(word_index) + 1  # +1 å› ä¸ºç´¢å¼•ä» 1 å¼€å§‹ï¼Œ0 æ˜¯ç”¨äºå¡«å……çš„
    embedding_matrix = np.zeros((vocab_size, embedding_dim))

    for word, i in word_index.items():
        if word in word2vec_model.wv:
            embedding_matrix[i] = word2vec_model.wv[word]

    return embedding_matrix

# ä¸»ç¨‹åº
if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    train_texts, train_labels, test_texts, test_labels = load_all_files()

    # æ•°æ®é¢„å¤„ç†ï¼šæ–‡æœ¬å‘é‡åŒ–
    MAX_NUM_WORDS = 10000  # è¯æ±‡è¡¨å¤§å°
    MAX_SEQ_LEN = 100  # æ¯ä¸ªå¥å­çš„æœ€å¤§é•¿åº¦

    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token="<OOV>")
    tokenizer.fit_on_texts(train_texts)

    X_train = tokenizer.texts_to_sequences(train_texts)
    X_test = tokenizer.texts_to_sequences(test_texts)

    # å¡«å……åºåˆ—åˆ°å›ºå®šé•¿åº¦
    X_train = pad_sequences(X_train, maxlen=MAX_SEQ_LEN, padding="post", truncating="post")
    X_test = pad_sequences(X_test, maxlen=MAX_SEQ_LEN, padding="post", truncating="post")

    y_train = np.array(train_labels)
    y_test = np.array(test_labels)

    # åŠ è½½é¢„è®­ç»ƒçš„ Word2Vec æ¨¡å‹
    word2vec_model = Word2Vec.load("./model/word2vec.model")
    EMBEDDING_DIM = word2vec_model.vector_size

    # åˆ›å»ºåµŒå…¥çŸ©é˜µ
    embedding_matrix = create_embedding_matrix(tokenizer.word_index, word2vec_model, EMBEDDING_DIM)

    # åˆ›å»ºåŒå±‚ LSTM æ¨¡å‹ï¼Œä½¿ç”¨ Word2Vec åµŒå…¥
    model = Sequential([
        Embedding(
            input_dim=embedding_matrix.shape[0],  # è¯æ±‡è¡¨å¤§å°
            output_dim=EMBEDDING_DIM,  # åµŒå…¥ç»´åº¦
            weights=[embedding_matrix],  # ä½¿ç”¨é¢„è®­ç»ƒåµŒå…¥çŸ©é˜µ
            input_length=MAX_SEQ_LEN,  # è¾“å…¥åºåˆ—é•¿åº¦
            trainable=False  # å†»ç»“åµŒå…¥å±‚
        ),
        LSTM(128, return_sequences=True),  # ç¬¬ä¸€å±‚ LSTMï¼Œè¿”å›åºåˆ—
        LSTM(128, return_sequences=False),  # ç¬¬äºŒå±‚ LSTMï¼Œä¸è¿”å›åºåˆ—
        Dropout(0.5),
        Dense(64, activation="relu"),
        Dropout(0.5),
        Dense(1, activation="sigmoid")  # äºŒåˆ†ç±»
    ])

    model.compile(
        optimizer="adam",
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )

    # è®­ç»ƒæ¨¡å‹
    BATCH_SIZE = 32
    EPOCHS = 5

    print("å¼€å§‹è®­ç»ƒåŒå±‚ LSTM æ¨¡å‹...")
    history = model.fit(
        X_train, y_train,
        validation_split=0.2,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        verbose=1
    )

    # æµ‹è¯•æ¨¡å‹
    print("\nè¯„ä¼°æ¨¡å‹...")
    y_pred = (model.predict(X_test) > 0.5).astype("int32")

    print("\nåˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(y_test, y_pred))

    print("\næ··æ·†çŸ©é˜µï¼š")
    print(confusion_matrix(y_test, y_pred))
```



ç›¸è¾ƒäºä¸€å±‚LSTMï¼ŒåŒå±‚LSTMçš„ä»£ç æ²¡å˜åŒ–å¤šå°‘

```py
LSTM(128, return_sequences=True),  # ç¬¬ä¸€å±‚ LSTMï¼Œè¿”å›åºåˆ—
LSTM(128, return_sequences=False),  # ç¬¬äºŒå±‚ LSTMï¼Œä¸è¿”å›åºåˆ—
```

è§£é‡Šä¸€ä¸‹

**`return_sequences=False`**:

- å½“è®¾ç½®ä¸º `False` æ—¶ï¼ŒLSTM å±‚åªä¼šè¿”å›è¾“å…¥åºåˆ—çš„**æœ€åä¸€ä¸ªæ—¶é—´æ­¥**çš„è¾“å‡ºã€‚è¿™æ ·åšé€šå¸¸é€‚ç”¨äºåºåˆ—çš„æœ€ç»ˆåˆ†ç±»æˆ–å›å½’ä»»åŠ¡ã€‚ï¼ˆæ‰€ä»¥è¿™é‡Œçš„ç¬¬äºŒå±‚LSTMè®¾ç½®æˆFalseï¼‰

**`return_sequences=True`**:

- å½“è®¾ç½®ä¸º `True` æ—¶ï¼ŒLSTM å±‚ä¼šè¿”å›**æ•´ä¸ªåºåˆ—**çš„è¾“å‡ºï¼ˆå³æ¯ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼‰ï¼Œè¿™åœ¨éœ€è¦è¿›ä¸€æ­¥å¤„ç†æ¯ä¸ªæ—¶é—´æ­¥çš„ä¿¡æ¯æ—¶éå¸¸æœ‰ç”¨ã€‚ï¼ˆæ‰€ä»¥è¿™é‡Œçš„ç¬¬ä¸€æ¬¡LSTMè®¾ç½®æˆTrueï¼Œè¿”å›æ•´ä¸ªåºåˆ—çš„è¾“å‡ºï¼‰



çœ‹çœ‹æ•ˆæœ

![image-20250103172548843](assets/4ILCflSpveNOgkh.png)

æå‡äº†5%





### ä¸€å±‚LSTM+Attentionæœºåˆ¶

```py
import os
import re
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
from gensim.models import Word2Vec
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

# æ¸…æ´—æ–‡æœ¬
def clean_text(text):
    text = re.sub(r'\d+', '', text)  # å»é™¤æ•°å­—
    text = re.sub(r'[^\w\s]', '', text)  # å»é™¤æ ‡ç‚¹ç¬¦å·
    text = text.lower()  # è½¬ä¸ºå°å†™
    return text

# åŠ è½½æ•°æ®
def load_files_from_dir(directory):
    texts = []
    labels = []
    for label in ["pos", "neg"]:
        label_dir = os.path.join(directory, label)
        for file_name in os.listdir(label_dir):
            with open(os.path.join(label_dir, file_name), "r", encoding="utf-8") as file:
                text = file.read()
                cleaned_text = clean_text(text)
                texts.append(cleaned_text)
                labels.append(1 if label == "pos" else 0)  # æ­£ç±»ä¸º 1ï¼Œè´Ÿç±»ä¸º 0
    return texts, labels

def load_all_files():
    train_texts, train_labels = load_files_from_dir("./aclImdb/train")
    test_texts, test_labels = load_files_from_dir("./aclImdb/test")
    return train_texts, train_labels, test_texts, test_labels

# åˆ›å»ºåµŒå…¥çŸ©é˜µ
def create_embedding_matrix(word_index, word2vec_model, embedding_dim):
    vocab_size = len(word_index) + 1  # +1 å› ä¸ºç´¢å¼•ä» 1 å¼€å§‹ï¼Œ0 æ˜¯ç”¨äºå¡«å……çš„
    embedding_matrix = np.zeros((vocab_size, embedding_dim))

    for word, i in word_index.items():
        if word in word2vec_model.wv:
            embedding_matrix[i] = word2vec_model.wv[word]

    return embedding_matrix

# ä¸»ç¨‹åº
if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    train_texts, train_labels, test_texts, test_labels = load_all_files()

    # æ•°æ®é¢„å¤„ç†ï¼šæ–‡æœ¬å‘é‡åŒ–
    MAX_NUM_WORDS = 10000  # è¯æ±‡è¡¨å¤§å°
    MAX_SEQ_LEN = 100  # æ¯ä¸ªå¥å­çš„æœ€å¤§é•¿åº¦

    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token="<OOV>")
    tokenizer.fit_on_texts(train_texts)

    X_train = tokenizer.texts_to_sequences(train_texts)
    X_test = tokenizer.texts_to_sequences(test_texts)

    # å¡«å……åºåˆ—åˆ°å›ºå®šé•¿åº¦
    X_train = pad_sequences(X_train, maxlen=MAX_SEQ_LEN, padding="post", truncating="post")
    X_test = pad_sequences(X_test, maxlen=MAX_SEQ_LEN, padding="post", truncating="post")

    y_train = np.array(train_labels)
    y_test = np.array(test_labels)

    # åŠ è½½é¢„è®­ç»ƒçš„ Word2Vec æ¨¡å‹
    word2vec_model = Word2Vec.load("./model/word2vec.model")
    EMBEDDING_DIM = word2vec_model.vector_size


    # åˆ›å»ºåµŒå…¥çŸ©é˜µ
    embedding_matrix = create_embedding_matrix(tokenizer.word_index, word2vec_model, EMBEDDING_DIM)



    # è‡ªæ³¨æ„åŠ›å±‚
    class AttentionLayer(tf.keras.layers.Layer):
        def __init__(self, **kwargs):
            super(AttentionLayer, self).__init__(**kwargs)

        def build(self, input_shape):
            self.W = self.add_weight(name="attention_weight", shape=(input_shape[-1], 1),
                                     initializer="random_normal", trainable=True)
            self.b = self.add_weight(name="attention_bias", shape=(1,),
                                     initializer="zeros", trainable=True)
            super(AttentionLayer, self).build(input_shape)

        def call(self, inputs):
            scores = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)
            attention_weights = tf.nn.softmax(scores, axis=1)
            context_vector = attention_weights * inputs
            context_vector = tf.reduce_sum(context_vector, axis=1)
            return context_vector


    # ä½¿ç”¨è‡ªå®šä¹‰ AttentionLayer
    model = Sequential([
        Embedding(
            input_dim=embedding_matrix.shape[0],  # è¯æ±‡è¡¨å¤§å°
            output_dim=EMBEDDING_DIM,  # åµŒå…¥ç»´åº¦
            weights=[embedding_matrix],  # ä½¿ç”¨é¢„è®­ç»ƒåµŒå…¥çŸ©é˜µ
            input_length=MAX_SEQ_LEN,  # è¾“å…¥åºåˆ—é•¿åº¦
            trainable=False  # å†»ç»“åµŒå…¥å±‚
        ),
        LSTM(128, return_sequences=True),  # è¿”å›åºåˆ—ä¾› Attention ä½¿ç”¨
        AttentionLayer(),  # æ·»åŠ è‡ªæ³¨æ„åŠ›æœºåˆ¶
        Dropout(0.5),
        Dense(64, activation="relu"),
        Dropout(0.5),
        Dense(1, activation="sigmoid")  # äºŒåˆ†ç±»
    ])

    model.compile(
        optimizer="adam",
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )

    # è®­ç»ƒæ¨¡å‹
    BATCH_SIZE = 32
    EPOCHS = 5
    print("å¼€å§‹è®­ç»ƒå¸¦æœ‰è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ LSTM æ¨¡å‹...")
    history = model.fit(
        X_train, y_train,
        validation_split=0.2,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        verbose=1
    )

    # æµ‹è¯•æ¨¡å‹
    print("\nè¯„ä¼°æ¨¡å‹...")
    y_pred = (model.predict(X_test) > 0.5).astype("int32")

    print("\nåˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(y_test, y_pred))

    print("\næ··æ·†çŸ©é˜µï¼š")
    print(confusion_matrix(y_test, y_pred))
```

è¿™ä¸ªæ¨¡å‹çš„æ ¸å¿ƒåœ¨äºAttentionå±‚

```py
    # è‡ªæ³¨æ„åŠ›å±‚
    class AttentionLayer(tf.keras.layers.Layer):
        def __init__(self, **kwargs):
            super(AttentionLayer, self).__init__(**kwargs)

        def build(self, input_shape):
            self.W = self.add_weight(name="attention_weight", shape=(input_shape[-1], 1),
                                     initializer="random_normal", trainable=True)
            self.b = self.add_weight(name="attention_bias", shape=(1,),
                                     initializer="zeros", trainable=True)
            super(AttentionLayer, self).build(input_shape)

        def call(self, inputs):
            scores = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)
            attention_weights = tf.nn.softmax(scores, axis=1)
            context_vector = attention_weights * inputs
            context_vector = tf.reduce_sum(context_vector, axis=1)
            return context_vector
```

è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¸ºæ¯ä¸ªè¾“å…¥åºåˆ—ä¸­çš„æ—¶é—´æ­¥è®¡ç®—ä¸€ä¸ªæ³¨æ„åŠ›æƒé‡ï¼Œè¿™äº›æƒé‡åæ˜ äº†å…¶ä»–æ—¶é—´æ­¥å¯¹äºè¯¥æ—¶é—´æ­¥çš„é‡è¦æ€§ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å‰ä¸€ç¯‡æ–‡ç« ä¸­æœ‰è¯¦ç»†è®²è¿°ï¼Œæƒ³äº†è§£å¯ä»¥çœ‹å¾€æœŸæ–‡ç« 



çœ‹çœ‹æ•ˆæœ

![æˆªå±2025-01-03 17.30.04](assets/TzUeW6Bm5gh8JuE.png)

åˆæé«˜1%



æœ€åæœ‰è¯•äº†ä¸€ä¸‹åŒå±‚LSTM+è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•ˆæœå˜å·®äº†ï¼Œä¸çŸ¥é“ä¸ºå•¥

è¿™æ˜¯ä¸ªå¾ˆæœ‰æ„æ€çš„åœ°æ–¹å“ˆ

æœ‰ä»¥ä¸‹å‡ ç§å¯èƒ½ï¼š

1.è®­ç»ƒè¶…å‚æ•°è®¾ç½®ä¸å½“ï¼ŒåŒå±‚ LSTM çš„æ¨¡å‹æ›´å¤æ‚ï¼Œå¯¹å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ç­‰è¶…å‚æ•°æ›´æ•æ„Ÿ

2.åŒå±‚LSTMå¤ªå¤æ‚ï¼Œè¿‡æ‹Ÿåˆäº†

3.Attention å±‚ä½œç”¨ï¼ŒAttention å±‚å·²ç»ä¸ºæ¨¡å‹æä¾›äº†æ›´å¼ºçš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå¯èƒ½å¯¼è‡´åŒå±‚ LSTM çš„é¢å¤–å¤æ‚æ€§æˆä¸ºç´¯èµ˜



é‚£æˆ‘ä»¬å°±ä¸€ä¸ªä¸€ä¸ªçš„æ’æŸ¥

çœ‹ä¸¤ä¸ªçš„è®­ç»ƒè¿‡ç¨‹

å•å±‚ï¼š

![æˆªå±2025-01-03 17.42.31](assets/trmSo52cHQAnlfK.png)

åŒå±‚ï¼š

![image-20250103174436165](assets/qI9rS6awXmYQR2k.png)



lossï¼šå¯¹**è®­ç»ƒé›†**çš„å¹³å‡æŸå¤±å€¼

accuracyï¼šå¯¹**è®­ç»ƒé›†**çš„å‡†ç¡®ç‡

val_lossï¼šå¯¹**éªŒè¯é›†**çš„å¹³å‡æŸå¤±å€¼

val_accuracyï¼šå¯¹**éªŒè¯é›†**çš„å‡†ç¡®ç‡



å…ˆçœ‹ç¬¬ä¸€ç§æƒ…å†µï¼š1.è®­ç»ƒè¶…å‚æ•°è®¾ç½®ä¸å½“ï¼ŒåŒå±‚ LSTM çš„æ¨¡å‹æ›´å¤æ‚ï¼Œå¯¹å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ç­‰è¶…å‚æ•°æ›´æ•æ„Ÿ

ç”±å›¾å¯ä»¥çœ‹åˆ°ï¼šä¸ç®¡æ˜¯åœ¨è®­ç»ƒé›†ä¸Šè¿˜æ˜¯åœ¨æµ‹è¯•é›†ä¸Šï¼Œå¹³å‡æŸå¤±å€¼éƒ½æ˜¯åœ¨ç¨³æ­¥ä¸‹é™çš„ï¼Œæ‰€ä»¥ä¸æ˜¯å‚æ•°é…ç½®ä¸å½“çš„åŸå› 



æ¥ç€çœ‹ç¬¬äºŒç§æƒ…å†µï¼š2.åŒå±‚LSTMå¤ªå¤æ‚ï¼Œè¿‡æ‹Ÿåˆäº†

å¯¹æ¯”ä¸¤å¼ å›¾å¯ä»¥çœ‹åˆ°åŒå±‚LSTMæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°æ˜¯æ›´å¥½çš„ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸Šå°±è¡¨ç°å¾—è¾ƒå·®

æ‰€ä»¥å¾ˆå¯èƒ½æ˜¯åŒå±‚LSTMå¤ªå¤æ‚ï¼Œè¿‡æ‹Ÿåˆäº†



æœ€åæ¥çœ‹çœ‹ç¬¬ä¸‰ç§æƒ…å†µï¼š3.Attention å±‚ä½œç”¨ï¼ŒAttention å±‚å·²ç»ä¸ºæ¨¡å‹æä¾›äº†æ›´å¼ºçš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå¯èƒ½å¯¼è‡´åŒå±‚ LSTM çš„é¢å¤–å¤æ‚æ€§æˆä¸ºç´¯èµ˜

é—®äº†ä¸€ä¸‹chatgptï¼Œå®ƒç»™å‡ºçš„æ–¹æ¡ˆæ˜¯ï¼šå°è¯•è°ƒæ•´ Attention å±‚çš„ä½ç½®ï¼Œä¾‹å¦‚æ”¾åœ¨ç¬¬ä¸€å±‚ LSTM åï¼Œé‚£å°±è¯•è¯•å‘—

```py
    # æ„å»ºæ¨¡å‹
    model = Sequential([
        Embedding(
            input_dim=embedding_matrix.shape[0],  
            output_dim=EMBEDDING_DIM,
            weights=[embedding_matrix],  
            input_length=MAX_SEQ_LEN, 
            trainable=False  
        ),
        LSTM(128, return_sequences=True),  # ç¬¬ä¸€å±‚ LSTMï¼Œè¿”å›åºåˆ—ä¾› Attention ä½¿ç”¨
        AttentionLayer(),  # Attention å±‚ï¼Œæå–ç¬¬ä¸€å±‚ LSTM çš„é‡è¦ç‰¹å¾
        tf.keras.layers.Reshape((1, 128)),  # è¿™æ ·çš„å˜å½¢æ“ä½œé€‚åˆå°†æ•°æ®ä¼ å…¥ LSTM å±‚
        LSTM(64, return_sequences=False),  # ç¬¬äºŒå±‚ LSTMï¼Œå¤„ç† Attention çš„é‡è¦ç‰¹å¾
        Dropout(0.5),
        Dense(64, activation="relu"),
        Dropout(0.5),
        Dense(1, activation="sigmoid")  # äºŒåˆ†ç±»
    ])
```

ä»¤æˆ‘æƒŠå–œçš„äº‹æƒ…å‘ç”Ÿå•¦

å‡†ç¡®ç‡ç«Ÿç„¶æé«˜äº†

![image-20250103175801206](assets/h8JRVHsob9qWYct.png)



å†çœ‹çœ‹è®­ç»ƒè¿‡ç¨‹ï¼š

![image-20250103180055914](assets/RBHNh1xCwzc9TFM.png)

å¯¹æ¯”ä¹‹å‰çš„å•å±‚LSTMï¼Œæ•ˆæœæ˜æ˜¾å˜å¥½



### æ€»ç»“

ç¡®å®æ˜¯æ²¡æƒ³åˆ°è¿˜èƒ½å†æå‡ä¸€ç‚¹ï¼Œèµ·åˆåªæ˜¯æŠ±ç€è¯•è¯•çœ‹çš„å¿ƒæ€

æœ€åè¯´ä¸€ä¸‹æˆ‘çš„çŒœæƒ³ï¼š

åœ¨ `LSTM â†’ LSTM â†’ Attention` ç»“æ„ä¸­ï¼š

- ç¬¬ä¸€å±‚ LSTM çš„è¾“å‡ºç›´æ¥ä¼ é€’ç»™ç¬¬äºŒå±‚ LSTMã€‚
- ç¬¬äºŒå±‚ LSTM ä¼šè¿›ä¸€æ­¥å¯¹æ—¶é—´åºåˆ—ç‰¹å¾è¿›è¡ŒæŠ½è±¡å’Œå‹ç¼©ï¼Œä½†å¯èƒ½ä¼šä¸¢å¤±ä¸€äº›æœ‰ç”¨çš„å±€éƒ¨ä¿¡æ¯ã€‚
- Attention åœ¨æœ€åä¸€æ­¥æ‰èƒ½ä½œç”¨äºæ•´ä¸ªåºåˆ—è¾“å‡ºï¼Œæ— æ³•æŒ½å›å·²ç»è¢« LSTM å±‚å‹ç¼©æˆ–å¿½ç•¥çš„ç»†èŠ‚

åœ¨ `LSTM â†’ Attention â†’ LSTM` ç»“æ„ä¸­ï¼š

- ç¬¬ä¸€å±‚ LSTM çš„è¾“å‡ºä¼šé€šè¿‡ Attention å±‚æå–é‡è¦çš„å±€éƒ¨ç‰¹å¾ï¼Œå¹¶ç”¨åŠ æƒæ–¹å¼èšç„¦äºå…³é”®å†…å®¹ã€‚
- ç¬¬äºŒå±‚ LSTM åªéœ€å¤„ç†è¿™äº›å·²ç»è¢«ç­›é€‰å’ŒåŠ æƒçš„å…³é”®ä¿¡æ¯ï¼Œå› æ­¤å¯ä»¥æ›´æœ‰æ•ˆåœ°å­¦ä¹ æ·±å±‚ç‰¹å¾



## **å†è¯´ç®€å•ä¸€ç‚¹å°±æ˜¯`LSTM â†’ Attention â†’ LSTM` é€šè¿‡æ—©æœŸå¼•å…¥ Attention èšç„¦å…³é”®ä¿¡æ¯ï¼Œå‡å°‘äº†ç‰¹å¾å†—ä½™å’Œä¿¡æ¯ä¸¢å¤±é—®é¢˜ï¼ŒåŒæ—¶æé«˜äº†è®¡ç®—æ•ˆç‡**
